{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION SHEET: Hands-On Big Data Processing with MapReduce\n",
    "\n",
    "This notebook contains the complete solutions for the MapReduce workshop exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's make sure you have the necessary libraries and data. Run the cell below to download the text for *Moby Dick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moby_dick.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "# Let's import the libraries we'll need for this workshop\n",
    "import time\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "file_path = 'moby_dick.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Downloading Moby Dick to {file_path}...\")\n",
    "    url = 'https://www.gutenberg.org/ebooks/2701.txt.utf-8'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(\"Download complete.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "else:\n",
    "    print(f\"{file_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Baseline Performance - Serial Word Count\n",
    "\n",
    "**Task:** Before optimizing, we need a baseline. Complete the `serial_word_count` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serial word count...\n",
      "Serial execution took: 0.1675 seconds\n",
      "10 most common words: [('the', 14715), ('and', 6514), ('that', 3081), ('his', 2530), ('but', 1822), ('with', 1769), ('was', 1647), ('for', 1644), ('all', 1543), ('this', 1437)]\n"
     ]
    }
   ],
   "source": [
    "def serial_word_count(file_path):\n",
    "    \"\"\"\n",
    "    Reads a text file and counts the frequency of words in a serial manner.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().lower()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please run the setup cell first.\")\n",
    "        return None\n",
    "\n",
    "    # SOLUTION: Find all words using a regular expression.\n",
    "    # The pattern r'\\b[a-z]{3,}\\b' is a good starting point.\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', text)\n",
    "\n",
    "    # SOLUTION: Use collections.Counter to count the words.\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "# --- Execution and Timing ---\n",
    "print(\"Starting serial word count...\")\n",
    "start_time_serial = time.perf_counter()\n",
    "s_word_counts = serial_word_count(file_path)\n",
    "end_time_serial = time.perf_counter()\n",
    "\n",
    "serial_execution_time = end_time_serial - start_time_serial\n",
    "\n",
    "print(f\"Serial execution took: {serial_execution_time:.4f} seconds\")\n",
    "# Optional: Print the 10 most common words to verify\n",
    "if s_word_counts:\n",
    "    print(\"10 most common words:\", s_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Parallel Processing with MapReduce\n",
    "\n",
    "**Task:** Now, implement the same word count logic using a MapReduce approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Prepare Data Chunks\n",
    "First, we need to split our data into chunks to be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 processes.\n",
      "Split text into 17 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of processes to use (usually the number of CPU cores)\n",
    "# You can change this number to see how it affects performance!\n",
    "NUM_PROCESSES = multiprocessing.cpu_count()\n",
    "print(f\"Using {NUM_PROCESSES} processes.\")\n",
    "\n",
    "def chunkify_text(file_path, num_chunks):\n",
    "    \"\"\"Splits the text into a specified number of chunks.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        chunk_size = len(text) // num_chunks\n",
    "        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        return chunks\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found.\")\n",
    "        return []\n",
    "\n",
    "# Create the chunks for our mappers\n",
    "text_chunks = chunkify_text(file_path, NUM_PROCESSES)\n",
    "if text_chunks:\n",
    "    print(f\"Split text into {len(text_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Implement the `Mapper` and `Reducer`\n",
    "\n",
    "**Task:** Complete the `mapper` and `reducer` functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapreduce_functions import mapper, reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(text_chunk):\n",
    "    \"\"\"\n",
    "    Processes a chunk of text, finds words, and returns a list of (word, 1) tuples.\n",
    "    \"\"\"\n",
    "    # SOLUTION: Convert the chunk to lowercase and find all words.\n",
    "    text_chunk = text_chunk.lower()\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', text_chunk) \n",
    "    \n",
    "    # SOLUTION: Create a list of (word, 1) tuples and return it.\n",
    "    mapped_results = [(word, 1) for word in words]\n",
    "    return mapped_results\n",
    "\n",
    "\n",
    "def reducer(item):\n",
    "    \"\"\"\n",
    "    Reduces a key and its list of values to a single value.\n",
    "    Takes one item from the shuffled list, e.g., ('whale', [1, 1, 1, 1]).\n",
    "    \"\"\"\n",
    "    word, counts = item\n",
    "    # SOLUTION: Return a tuple of the word and the sum of its counts.\n",
    "    return (word, sum(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Orchestrate Map, Shuffle, and Reduce\n",
    "\n",
    "**Task:** Now, put all the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MapReduce word count...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-10:\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-9:\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process SpawnPoolWorker-12:\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-28:\n",
      "Process SpawnPoolWorker-26:\n",
      "Process SpawnPoolWorker-31:\n",
      "Process SpawnPoolWorker-27:\n",
      "Process SpawnPoolWorker-33:\n",
      "Process SpawnPoolWorker-30:\n",
      "Process SpawnPoolWorker-29:\n",
      "Process SpawnPoolWorker-32:\n",
      "Process SpawnPoolWorker-25:\n",
      "Process SpawnPoolWorker-22:\n",
      "Process SpawnPoolWorker-24:\n",
      "Process SpawnPoolWorker-21:\n",
      "Process SpawnPoolWorker-19:\n",
      "Process SpawnPoolWorker-23:\n",
      "Process SpawnPoolWorker-20:\n",
      "Process SpawnPoolWorker-18:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# --- MAP STAGE ---\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a pool of worker processes\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mNUM_PROCESSES) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# This applies the mapper function to each chunk in parallel\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     mapped_results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(mapper, text_chunks)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# --- SHUFFLE STAGE ---\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# The result is a list of lists; we need to flatten it and group by key.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m shuffled_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Initialization ---\n",
    "# Initialize an empty dictionary to store the final word counts from the MapReduce process.\n",
    "mr_word_counts = {}\n",
    "# Initialize a variable to hold the total execution time. We set it to 0 in case the process is skipped.\n",
    "mapreduce_execution_time = 0\n",
    "\n",
    "# --- Safety Check ---\n",
    "# This 'if' statement is a safety check. It ensures that the MapReduce logic only runs\n",
    "# if the 'text_chunks' list was successfully created in the previous step.\n",
    "# If the file wasn't found or was empty, text_chunks would be empty or None, and this block would be skipped.\n",
    "if text_chunks:\n",
    "    print(\"Starting MapReduce word count...\")\n",
    "    # Record the starting time using a high-precision performance counter.\n",
    "    start_time_mapreduce = time.perf_counter()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # --- MAP STAGE: Process data chunks in parallel ---\n",
    "    # --------------------------------------------------------------------------\n",
    "    # A 'Pool' is an object that manages a pool of worker processes.\n",
    "    # 'NUM_PROCESSES' is typically set to the number of CPU cores on the machine.\n",
    "    # The 'with' statement ensures that all processes in the pool are properly closed down\n",
    "    # once the work is done, even if errors occur.\n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        # 'pool.map()' is the core of the parallel execution. It does three things:\n",
    "        # 1. It takes the iterable 'text_chunks' and splits it up among the worker processes in the pool.\n",
    "        # 2. It applies the 'mapper' function to each individual chunk in parallel.\n",
    "        # 3. It waits for all workers to finish and collects their results into a single list.\n",
    "        # The result, 'mapped_results', will be a list of lists, e.g., [[('the', 1), ('a', 1)], [('whale', 1), ('the', 1)], ...]\n",
    "        mapped_results = pool.map(mapper, text_chunks)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # --- SHUFFLE STAGE: Group the results by key (word) ---\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 'defaultdict(list)' is a convenient type of dictionary. If you try to access or add to a key\n",
    "    # that doesn't exist, it automatically creates that key with a default value, which is an empty list [] in this case.\n",
    "    # This is perfect for grouping items without needing to check if the key exists first.\n",
    "    shuffled_data = defaultdict(list)\n",
    "    \n",
    "    # This nested loop iterates through the 'mapped_results' to group them.\n",
    "    # The outer loop goes through each list returned by a mapper process (e.g., [('whale', 1), ('the', 1)]).\n",
    "    for result_list in mapped_results:\n",
    "        # The inner loop goes through each (key, value) tuple in that list (e.g., ('whale', 1)).\n",
    "        for key, value in result_list:\n",
    "            # For each key (the word), it appends the value (the number 1) to its list in the defaultdict.\n",
    "            # After this loop, shuffled_data will look like: {'the': [1, 1, 1,...], 'whale': [1, 1,...], ...}\n",
    "            shuffled_data[key].append(value)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # --- REDUCE STAGE: Aggregate the grouped values in parallel ---\n",
    "    # --------------------------------------------------------------------------\n",
    "    # We create another pool of worker processes for the reduce step.\n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        # We again use 'pool.map()'. This time, we apply the 'reducer' function.\n",
    "        # 'shuffled_data.items()' provides an iterable of (key, value_list) pairs,\n",
    "        # for example: ('the', [1, 1, 1, ...]).\n",
    "        # Each of these items is sent to a worker process to be \"reduced\" (in this case, summed).\n",
    "        reduced_results = pool.map(reducer, shuffled_data.items())\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # --- Finalization ---\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 'reduced_results' is currently a list of tuples, e.g., [('the', 14715), ('whale', 1234), ...].\n",
    "    # The 'dict()' constructor efficiently converts this list of key-value pairs into a Python dictionary.\n",
    "    mr_word_counts = dict(reduced_results)\n",
    "\n",
    "    # Record the ending time.\n",
    "    end_time_mapreduce = time.perf_counter()\n",
    "    # Calculate the total duration of the MapReduce process.\n",
    "    mapreduce_execution_time = end_time_mapreduce - start_time_mapreduce\n",
    "\n",
    "    print(f\"MapReduce execution took: {mapreduce_execution_time:.4f} seconds\")\n",
    "\n",
    "    # This is an optional but important verification step.\n",
    "    if mr_word_counts:\n",
    "        # We sort the final dictionary by its values (the word counts) in descending order to find the most frequent words.\n",
    "        # 'sorted()' returns a list of (key, value) tuples.\n",
    "        # 'key=lambda item: item[1]' tells the sort function to use the second element of each tuple (the count) for sorting.\n",
    "        sorted_counts = sorted(mr_word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        # We print the top 10 results to visually confirm they are sensible and likely match the serial version's output.\n",
    "        print(\"10 most common words:\", sorted_counts[:10])\n",
    "else:\n",
    "    # This message is printed if the initial 'if text_chunks:' check failed.\n",
    "    print(\"Skipping MapReduce execution because data chunks were not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Performance Comparison and Visualization\n",
    "\n",
    "**Task:** Now that you have timings for both methods, create a bar chart to visually compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for plotting\n",
    "methods = ['Serial Processing', f'MapReduce ({NUM_PROCESSES} Cores)']\n",
    "times = [serial_execution_time, mapreduce_execution_time]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(methods, times, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Performance Comparison: Serial vs. MapReduce')\n",
    "\n",
    "# Add the time values on top of the bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.4f}s', va='bottom', ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the speedup factor\n",
    "if mapreduce_execution_time > 0:\n",
    "    speedup = serial_execution_time / mapreduce_execution_time\n",
    "    print(f\"\\nMapReduce was {speedup:.2f}x faster than the serial implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Conceptual Questions on Hadoop\n",
    "\n",
    "**Task:** Based on your reading from Part B of the handout, answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions & Solutions:\n",
    "\n",
    "**1. What is the Hadoop Distributed File System (HDFS)? What problem does it solve?**\n",
    "\n",
    "*__Solution:__ HDFS is a specialized filesystem designed to store and manage massive datasets (terabytes or petabytes) across a large cluster of commodity (standard, inexpensive) computers. It solves the problem of storing data that is too large to fit on a single machine. It does this by splitting large files into smaller blocks and distributing those blocks across many computers in the cluster. It also provides fault tolerance by replicating each block multiple times on different machines, so if one machine fails, the data is not lost.*\n",
    "\n",
    "**2. What are the roles of the `NameNode` and `DataNode` in HDFS?**\n",
    "\n",
    "*__Solution:__ HDFS has a master/worker architecture:\n",
    "- **NameNode (Master):** The NameNode is the brain of the filesystem. It does not store the actual data itself. Instead, it holds the metadata, which is like a table of contents. This metadata includes the file directory structure, permissions, and, most importantly, the location of all the data blocks for every file in the cluster. All requests to read or write a file go to the NameNode first.\n",
    "- **DataNode (Worker):** DataNodes are the workhorses. They are responsible for storing the actual data blocks on their local disks. They report back to the NameNode periodically with a list of the blocks they are storing to prove they are alive and well (this is called a \"heartbeat\").*\n",
    "\n",
    "**3. What is YARN (Yet Another Resource Negotiator)? What is its role in the Hadoop ecosystem?**\n",
    "\n",
    "*__Solution:__ YARN is the resource management and job scheduling layer of Hadoop. It acts as the \"operating system\" for the entire Hadoop cluster. Its primary role is to manage the cluster's resources (CPU, memory) and allocate them to various applications. Before YARN, MapReduce was tightly coupled with resource management. YARN separated these, allowing different data processing frameworks (like Apache Spark, Flink, and MapReduce itself) to run simultaneously on the same Hadoop cluster, sharing resources efficiently.*\n",
    "\n",
    "**4. How does running MapReduce on Hadoop differ from our simple Python implementation?**\n",
    "\n",
    "*__Solution:__ There are several key differences:\n",
    "1. **Scale:** Our Python script runs on a single machine, limited by its CPU cores and RAM. Hadoop runs on a cluster of tens to thousands of machines, enabling analysis of vastly larger datasets.\n",
    "2. **Data Storage:** Our script reads from a local file. Hadoop's MapReduce reads data directly from HDFS, where the data is already distributed across the cluster.\n",
    "3. **Fault Tolerance:** If our Python script fails (or the computer crashes), the job stops and must be restarted manually. Hadoop is designed for failure; if a DataNode or a specific task fails, YARN will automatically reschedule the task on another available machine.\n",
    "4. **Data Locality:** Our script moves data from the disk to the CPU. Hadoop is built on the principle of moving the *computation to the data*. YARN tries to schedule MapReduce tasks on the same DataNodes where the data blocks reside, minimizing network traffic and drastically improving efficiency.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Looking Ahead - Final Thoughts\n",
    "\n",
    "**Task:** In the markdown cell below, briefly brainstorm one or two ideas for how you could use the MapReduce pattern in your next major assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Ideas for the Next Assignment:\n",
    "\n",
    "**Dataset Idea:** *A large dataset of millions of e-commerce customer reviews.*\n",
    "\n",
    "**MapReduce Application:** *The goal is to find the average rating and number of reviews for every unique product ID. A serial loop would be very slow.\n",
    "- **Map:** The mapper would process each row (review). For each review, it would emit a key-value pair of `(product_id, (rating, 1))`. The rating is the score, and the 1 is to count the review.\n",
    "- **Reduce:** The reducer would receive a product ID and a list of tuples, e.g., `(product_123, [(5, 1), (4, 1), (5, 1)])`. It would then calculate the sum of ratings and the sum of the counts to produce a final output: `(product_123, (average_rating, total_reviews))`. This pre-computed data makes visualizing top/bottom products much faster.*\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Idea:** *A year's worth of server access logs from a high-traffic website (billions of lines).*\n",
    "\n",
    "**MapReduce Application:** *The goal is to identify which IP addresses are accessing the site most frequently, potentially to detect bots or scraping activity.\n",
    "- **Map:** The mapper would parse each log line. It would extract the client IP address and emit a key-value pair of `(ip_address, 1)`.\n",
    "- **Reduce:** The reducer would receive an IP address and a list of ones, e.g., `(192.168.1.1, [1, 1, 1, ...])`. It would simply sum the list to get the total number of hits for that IP. The final output can then be sorted to find the top IP addresses.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
