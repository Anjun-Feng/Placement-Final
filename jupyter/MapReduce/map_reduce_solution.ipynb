{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION SHEET: Hands-On Big Data Processing with MapReduce\n",
    "\n",
    "This notebook contains the complete solutions for the MapReduce workshop exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's make sure you have the necessary libraries and data. Run the cell below to download the text for *Moby Dick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moby_dick.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "# Let's import the libraries we'll need for this workshop\n",
    "import time\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "file_path = 'moby_dick.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Downloading Moby Dick to {file_path}...\")\n",
    "    url = 'https://www.gutenberg.org/ebooks/2701.txt.utf-8'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(\"Download complete.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "else:\n",
    "    print(f\"{file_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Baseline Performance - Serial Word Count\n",
    "\n",
    "**Task:** Before optimizing, we need a baseline. Complete the `serial_word_count` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serial word count...\n",
      "Serial execution took: 0.1040 seconds\n",
      "10 most common words: [('the', 14715), ('and', 6514), ('that', 3081), ('his', 2530), ('but', 1822), ('with', 1769), ('was', 1647), ('for', 1644), ('all', 1543), ('this', 1437)]\n"
     ]
    }
   ],
   "source": [
    "def serial_word_count(file_path):\n",
    "    \"\"\"\n",
    "    Reads a text file and counts the frequency of words in a serial manner.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().lower()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please run the setup cell first.\")\n",
    "        return None\n",
    "\n",
    "    # SOLUTION: Find all words using a regular expression.\n",
    "    # The pattern r'\\b[a-z]{3,}\\b' is a good starting point.\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', text)\n",
    "\n",
    "    # SOLUTION: Use collections.Counter to count the words.\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "# --- Execution and Timing ---\n",
    "print(\"Starting serial word count...\")\n",
    "start_time_serial = time.perf_counter()\n",
    "s_word_counts = serial_word_count(file_path)\n",
    "end_time_serial = time.perf_counter()\n",
    "\n",
    "serial_execution_time = end_time_serial - start_time_serial\n",
    "\n",
    "print(f\"Serial execution took: {serial_execution_time:.4f} seconds\")\n",
    "# Optional: Print the 10 most common words to verify\n",
    "if s_word_counts:\n",
    "    print(\"10 most common words:\", s_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Parallel Processing with MapReduce\n",
    "\n",
    "**Task:** Now, implement the same word count logic using a MapReduce approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Prepare Data Chunks\n",
    "First, we need to split our data into chunks to be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 processes.\n",
      "Split text into 17 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of processes to use (usually the number of CPU cores)\n",
    "# You can change this number to see how it affects performance!\n",
    "NUM_PROCESSES = multiprocessing.cpu_count()\n",
    "print(f\"Using {NUM_PROCESSES} processes.\")\n",
    "\n",
    "def chunkify_text(file_path, num_chunks):\n",
    "    \"\"\"Splits the text into a specified number of chunks.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        chunk_size = len(text) // num_chunks\n",
    "        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        return chunks\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found.\")\n",
    "        return []\n",
    "\n",
    "# Create the chunks for our mappers\n",
    "text_chunks = chunkify_text(file_path, NUM_PROCESSES)\n",
    "if text_chunks:\n",
    "    print(f\"Split text into {len(text_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Implement the `Mapper` and `Reducer`\n",
    "\n",
    "**Task:** Complete the `mapper` and `reducer` functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapreduce_functions import mapper, reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(text_chunk):\n",
    "    \"\"\"\n",
    "    Processes a chunk of text, finds words, and returns a list of (word, 1) tuples.\n",
    "    \"\"\"\n",
    "    # SOLUTION: Convert the chunk to lowercase and find all words.\n",
    "    text_chunk = text_chunk.lower()\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', text_chunk)\n",
    "    \n",
    "    # SOLUTION: Create a list of (word, 1) tuples and return it.\n",
    "    mapped_results = [(word, 1) for word in words]\n",
    "    return mapped_results\n",
    "\n",
    "\n",
    "def reducer(item):\n",
    "    \"\"\"\n",
    "    Reduces a key and its list of values to a single value.\n",
    "    Takes one item from the shuffled list, e.g., ('whale', [1, 1, 1, 1]).\n",
    "    \"\"\"\n",
    "    word, counts = item\n",
    "    # SOLUTION: Return a tuple of the word and the sum of its counts.\n",
    "    return (word, sum(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Orchestrate Map, Shuffle, and Reduce\n",
    "\n",
    "**Task:** Now, put all the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MapReduce word count...\n",
      "MapReduce execution took: 0.7101 seconds\n",
      "10 most common words: [('the', 14715), ('and', 6513), ('that', 3081), ('his', 2530), ('but', 1822), ('with', 1769), ('was', 1647), ('for', 1644), ('all', 1543), ('this', 1437)]\n"
     ]
    }
   ],
   "source": [
    "# --- Execution and Timing ---\n",
    "mr_word_counts = {}\n",
    "mapreduce_execution_time = 0\n",
    "if text_chunks: # Only run if chunks were created successfully\n",
    "    print(\"Starting MapReduce word count...\")\n",
    "    start_time_mapreduce = time.perf_counter()\n",
    "\n",
    "    # --- MAP STAGE ---\n",
    "    # Create a pool of worker processes\n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        # This applies the mapper function to each chunk in parallel\n",
    "        mapped_results = pool.map(mapper, text_chunks)\n",
    "\n",
    "    # --- SHUFFLE STAGE ---\n",
    "    # The result is a list of lists; we need to flatten it and group by key.\n",
    "    shuffled_data = defaultdict(list)\n",
    "    # Flatten the list of lists into a single list of tuples\n",
    "    for result_list in mapped_results:\n",
    "        for key, value in result_list:\n",
    "            shuffled_data[key].append(value)\n",
    "\n",
    "    # --- REDUCE STAGE ---\n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        # pool.map works on an iterable, so we pass shuffled_data.items()\n",
    "        reduced_results = pool.map(reducer, shuffled_data.items())\n",
    "\n",
    "    # Final result is a list of tuples, which can be converted to a dictionary\n",
    "    mr_word_counts = dict(reduced_results)\n",
    "\n",
    "    end_time_mapreduce = time.perf_counter()\n",
    "    mapreduce_execution_time = end_time_mapreduce - start_time_mapreduce\n",
    "\n",
    "    print(f\"MapReduce execution took: {mapreduce_execution_time:.4f} seconds\")\n",
    "\n",
    "    # Optional: Print the 10 most common words to verify they match the serial version\n",
    "    if mr_word_counts:\n",
    "        # Sort the dictionary by value to find the most common words\n",
    "        sorted_counts = sorted(mr_word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        print(\"10 most common words:\", sorted_counts[:10])\n",
    "else:\n",
    "    print(\"Skipping MapReduce execution because data chunks were not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Performance Comparison and Visualization\n",
    "\n",
    "**Task:** Now that you have timings for both methods, create a bar chart to visually compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for plotting\n",
    "methods = ['Serial Processing', f'MapReduce ({NUM_PROCESSES} Cores)']\n",
    "times = [serial_execution_time, mapreduce_execution_time]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(methods, times, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Performance Comparison: Serial vs. MapReduce')\n",
    "\n",
    "# Add the time values on top of the bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.4f}s', va='bottom', ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the speedup factor\n",
    "if mapreduce_execution_time > 0:\n",
    "    speedup = serial_execution_time / mapreduce_execution_time\n",
    "    print(f\"\\nMapReduce was {speedup:.2f}x faster than the serial implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Conceptual Questions on Hadoop\n",
    "\n",
    "**Task:** Based on your reading from Part B of the handout, answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions & Solutions:\n",
    "\n",
    "**1. What is the Hadoop Distributed File System (HDFS)? What problem does it solve?**\n",
    "\n",
    "*__Solution:__ HDFS is a specialized filesystem designed to store and manage massive datasets (terabytes or petabytes) across a large cluster of commodity (standard, inexpensive) computers. It solves the problem of storing data that is too large to fit on a single machine. It does this by splitting large files into smaller blocks and distributing those blocks across many computers in the cluster. It also provides fault tolerance by replicating each block multiple times on different machines, so if one machine fails, the data is not lost.*\n",
    "\n",
    "**2. What are the roles of the `NameNode` and `DataNode` in HDFS?**\n",
    "\n",
    "*__Solution:__ HDFS has a master/worker architecture:\n",
    "- **NameNode (Master):** The NameNode is the brain of the filesystem. It does not store the actual data itself. Instead, it holds the metadata, which is like a table of contents. This metadata includes the file directory structure, permissions, and, most importantly, the location of all the data blocks for every file in the cluster. All requests to read or write a file go to the NameNode first.\n",
    "- **DataNode (Worker):** DataNodes are the workhorses. They are responsible for storing the actual data blocks on their local disks. They report back to the NameNode periodically with a list of the blocks they are storing to prove they are alive and well (this is called a \"heartbeat\").*\n",
    "\n",
    "**3. What is YARN (Yet Another Resource Negotiator)? What is its role in the Hadoop ecosystem?**\n",
    "\n",
    "*__Solution:__ YARN is the resource management and job scheduling layer of Hadoop. It acts as the \"operating system\" for the entire Hadoop cluster. Its primary role is to manage the cluster's resources (CPU, memory) and allocate them to various applications. Before YARN, MapReduce was tightly coupled with resource management. YARN separated these, allowing different data processing frameworks (like Apache Spark, Flink, and MapReduce itself) to run simultaneously on the same Hadoop cluster, sharing resources efficiently.*\n",
    "\n",
    "**4. How does running MapReduce on Hadoop differ from our simple Python implementation?**\n",
    "\n",
    "*__Solution:__ There are several key differences:\n",
    "1. **Scale:** Our Python script runs on a single machine, limited by its CPU cores and RAM. Hadoop runs on a cluster of tens to thousands of machines, enabling analysis of vastly larger datasets.\n",
    "2. **Data Storage:** Our script reads from a local file. Hadoop's MapReduce reads data directly from HDFS, where the data is already distributed across the cluster.\n",
    "3. **Fault Tolerance:** If our Python script fails (or the computer crashes), the job stops and must be restarted manually. Hadoop is designed for failure; if a DataNode or a specific task fails, YARN will automatically reschedule the task on another available machine.\n",
    "4. **Data Locality:** Our script moves data from the disk to the CPU. Hadoop is built on the principle of moving the *computation to the data*. YARN tries to schedule MapReduce tasks on the same DataNodes where the data blocks reside, minimizing network traffic and drastically improving efficiency.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Looking Ahead - Final Thoughts\n",
    "\n",
    "**Task:** In the markdown cell below, briefly brainstorm one or two ideas for how you could use the MapReduce pattern in your next major assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Ideas for the Next Assignment:\n",
    "\n",
    "**Dataset Idea:** *A large dataset of millions of e-commerce customer reviews.*\n",
    "\n",
    "**MapReduce Application:** *The goal is to find the average rating and number of reviews for every unique product ID. A serial loop would be very slow.\n",
    "- **Map:** The mapper would process each row (review). For each review, it would emit a key-value pair of `(product_id, (rating, 1))`. The rating is the score, and the 1 is to count the review.\n",
    "- **Reduce:** The reducer would receive a product ID and a list of tuples, e.g., `(product_123, [(5, 1), (4, 1), (5, 1)])`. It would then calculate the sum of ratings and the sum of the counts to produce a final output: `(product_123, (average_rating, total_reviews))`. This pre-computed data makes visualizing top/bottom products much faster.*\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Idea:** *A year's worth of server access logs from a high-traffic website (billions of lines).*\n",
    "\n",
    "**MapReduce Application:** *The goal is to identify which IP addresses are accessing the site most frequently, potentially to detect bots or scraping activity.\n",
    "- **Map:** The mapper would parse each log line. It would extract the client IP address and emit a key-value pair of `(ip_address, 1)`.\n",
    "- **Reduce:** The reducer would receive an IP address and a list of ones, e.g., `(192.168.1.1, [1, 1, 1, ...])`. It would simply sum the list to get the total number of hits for that IP. The final output can then be sorted to find the top IP addresses.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
