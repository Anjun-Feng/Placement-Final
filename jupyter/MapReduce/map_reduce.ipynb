{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Processing with MapReduce\n",
    "\n",
    "This notebook will guide you through the practical implementation and analysis of the MapReduce algorithm. You will start with a simple, single-threaded approach, build a parallel version, and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's make sure you have the necessary libraries and data. Run the cell below to download the text for *Moby Dick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Moby Dick to moby_dick.txt...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "# Let's import the libraries we'll need for this workshop\n",
    "import time\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "file_path = 'moby_dick.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Downloading Moby Dick to {file_path}...\")\n",
    "    url = 'https://www.gutenberg.org/ebooks/2701.txt.utf-8'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(\"Download complete.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "else:\n",
    "    print(f\"{file_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Baseline Performance - Serial Word Count\n",
    "\n",
    "**Task:** Before optimizing, we need a baseline. Complete the `serial_word_count` function below. It should:\n",
    "1.  Read the entire content of the `moby_dick.txt` file.\n",
    "2.  Use regular expressions (`re.findall`) to find all words. A good pattern is `r'\\b[a-z]{3,}\\b'`, which finds words of 3 or more lowercase letters. Remember to convert the text to lowercase first.\n",
    "3.  Use `collections.Counter` to count the frequency of each word.\n",
    "4.  Return the Counter object.\n",
    "\n",
    "The code to time and execute the function is already provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serial word count...\n",
      "Serial execution took: 0.1498 seconds\n",
      "10 most common words: [('the', 14715), ('and', 6514), ('that', 3081), ('his', 2530), ('but', 1822), ('with', 1769), ('was', 1647), ('for', 1644), ('all', 1543), ('this', 1437)]\n"
     ]
    }
   ],
   "source": [
    "def serial_word_count(file_path):\n",
    "    \"\"\"\n",
    "    Reads a text file and counts the frequency of words in a serial manner.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().lower()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please run the setup cell first.\")\n",
    "        return None\n",
    "\n",
    "    # TODO: Find all words using a regular expression.\n",
    "    # The pattern r'\\b[a-z]{3,}\\b' is a good starting point.\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', text)\n",
    "\n",
    "    # TODO: Use collections.Counter to count the words.\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "# --- Execution and Timing ---\n",
    "print(\"Starting serial word count...\")\n",
    "start_time_serial = time.perf_counter()\n",
    "s_word_counts = serial_word_count(file_path)\n",
    "end_time_serial = time.perf_counter()\n",
    "\n",
    "serial_execution_time = end_time_serial - start_time_serial\n",
    "\n",
    "print(f\"Serial execution took: {serial_execution_time:.4f} seconds\")\n",
    "# Optional: Print the 10 most common words to verify\n",
    "if s_word_counts:\n",
    "    print(\"10 most common words:\", s_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Parallel Processing with MapReduce\n",
    "\n",
    "**Task:** Now, implement the same word count logic using a MapReduce approach. We will use the `multiprocessing` library to parallelize the \"Map\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Prepare Data Chunks\n",
    "First, we need to split our data into chunks to be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 processes.\n",
      "Split text into 17 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of processes to use (usually the number of CPU cores)\n",
    "# You can change this number to see how it affects performance!\n",
    "NUM_PROCESSES = multiprocessing.cpu_count()\n",
    "print(f\"Using {NUM_PROCESSES} processes.\")\n",
    "\n",
    "def chunkify_text(file_path, num_chunks):\n",
    "    \"\"\"Splits the text into a specified number of chunks.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        chunk_size = len(text) // num_chunks\n",
    "        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        return chunks\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found.\")\n",
    "        return []\n",
    "\n",
    "# Create the chunks for our mappers\n",
    "text_chunks = chunkify_text(file_path, NUM_PROCESSES)\n",
    "if text_chunks:\n",
    "    print(f\"Split text into {len(text_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Implement the `Mapper` and `Reducer`\n",
    "\n",
    "**Task:** Complete the `mapper` and `reducer` functions below.\n",
    "* **`mapper`**: This function will receive one chunk of text. It should perform the same logic as the serial version (lowercase, find words) but instead of counting, it should return a list of `(word, 1)` tuples.\n",
    "* **`reducer`**: This function takes the grouped (shuffled) data, which is a list of `(word, [1, 1, 1, ...])` tuples. It should sum the list of ones for each word and return the final `(word, count)` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(text_chunk):\n",
    "    \"\"\"\n",
    "    Processes a chunk of text, finds words, and returns a list of (word, 1) tuples.\n",
    "    \"\"\"\n",
    "    # TODO: Convert the chunk to lowercase and find all words.\n",
    "    text_chunk = text_chunk.lower()\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', text_chunk)\n",
    "    \n",
    "    # TODO: Create a list of (word, 1) tuples and return it.\n",
    "    mapped_results = [(word, 1) for word in words]\n",
    "    return mapped_results\n",
    "\n",
    "\n",
    "def reducer(item):\n",
    "    \"\"\"\n",
    "    Reduces a key and its list of values to a single value.\n",
    "    Takes one item from the shuffled list, e.g., ('whale', [1, 1, 1, 1]).\n",
    "    \"\"\"\n",
    "    word, counts = item\n",
    "    # TODO: Return a tuple of the word and the sum of its counts.\n",
    "    return (word, sum(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Orchestrate Map, Shuffle, and Reduce\n",
    "\n",
    "**Task:** Now, put all the pieces together. The steps are:\n",
    "1.  **Map:** Use `multiprocessing.Pool` to apply the `mapper` function to each `text_chunk` in parallel.\n",
    "2.  **Shuffle:** The results from the mappers will be a list of lists (e.g., `[ [('a', 1)], [('b', 1), ('a', 1)] ]`). You need to collect all these `(key, value)` pairs and group them by key. A `defaultdict(list)` is perfect for this.\n",
    "3.  **Reduce:** Use the pool to apply the `reducer` function to the shuffled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MapReduce word count...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Process SpawnPoolWorker-2:\n",
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-4:\n",
      "Process SpawnPoolWorker-5:\n",
      "Process SpawnPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-7:\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-9:\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    }
   ],
   "source": [
    "# --- Execution and Timing ---\n",
    "mr_word_counts = {}\n",
    "mapreduce_execution_time = 0\n",
    "if text_chunks: # Only run if chunks were created successfully\n",
    "    print(\"Starting MapReduce word count...\")\n",
    "    start_time_mapreduce = time.perf_counter()\n",
    "\n",
    "    # --- MAP STAGE ---\n",
    "    # Create a pool of worker processes\n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        # This applies the mapper function to each chunk in parallel\n",
    "        mapped_results = pool.map(mapper, text_chunks)\n",
    "\n",
    "    # --- SHUFFLE STAGE ---\n",
    "    # The result is a list of lists; we need to flatten it and group by key.\n",
    "    shuffled_data = defaultdict(list)\n",
    "    # Flatten the list of lists into a single list of tuples\n",
    "    for result_list in mapped_results:\n",
    "        for key, value in result_list:\n",
    "            shuffled_data[key].append(value)\n",
    "\n",
    "    # --- REDUCE STAGE ---\n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        # pool.map works on an iterable, so we pass shuffled_data.items()\n",
    "        reduced_results = pool.map(reducer, shuffled_data.items())\n",
    "\n",
    "    # Final result is a list of tuples, which can be converted to a dictionary\n",
    "    mr_word_counts = dict(reduced_results)\n",
    "\n",
    "    end_time_mapreduce = time.perf_counter()\n",
    "    mapreduce_execution_time = end_time_mapreduce - start_time_mapreduce\n",
    "\n",
    "    print(f\"MapReduce execution took: {mapreduce_execution_time:.4f} seconds\")\n",
    "\n",
    "    # Optional: Print the 10 most common words to verify they match the serial version\n",
    "    if mr_word_counts:\n",
    "        # Sort the dictionary by value to find the most common words\n",
    "        sorted_counts = sorted(mr_word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        print(\"10 most common words:\", sorted_counts[:10])\n",
    "else:\n",
    "    print(\"Skipping MapReduce execution because data chunks were not created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Performance Comparison and Visualization\n",
    "\n",
    "**Task:** Now that you have timings for both methods, create a bar chart to visually compare them.\n",
    "\n",
    "1.  Store your two timing results (`serial_execution_time` and `mapreduce_execution_time`).\n",
    "2.  Use `matplotlib.pyplot` to create a bar chart showing the difference.\n",
    "3.  Label your chart and axes clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for plotting\n",
    "methods = ['Serial Processing', f'MapReduce ({NUM_PROCESSES} Cores)']\n",
    "times = [serial_execution_time, mapreduce_execution_time]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(methods, times, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Performance Comparison: Serial vs. MapReduce')\n",
    "\n",
    "# Add the time values on top of the bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.4f}s', va='bottom', ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the speedup factor\n",
    "if mapreduce_execution_time > 0:\n",
    "    speedup = serial_execution_time / mapreduce_execution_time\n",
    "    print(f\"\\nMapReduce was {speedup:.2f}x faster than the serial implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Conceptual Questions on Hadoop\n",
    "\n",
    "**Task:** Based on your reading from Part B of the handout, answer the following questions in the markdown cell below. Double-click the cell to edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "**1. What is the Hadoop Distributed File System (HDFS)? What problem does it solve?**\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "**2. What are the roles of the `NameNode` and `DataNode` in HDFS?**\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "**3. What is YARN (Yet Another Resource Negotiator)? What is its role in the Hadoop ecosystem?**\n",
    "\n",
    "*Your answer here.*\n",
    "\n",
    "**4. How does running MapReduce on Hadoop differ from our simple Python implementation?**\n",
    "\n",
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Looking Ahead - Final Thoughts\n",
    "\n",
    "**Task:** In the markdown cell below, briefly brainstorm one or two ideas for how you could use the MapReduce pattern in your next major assignment. Think about the dataset you might use and what kind of pre-processing or aggregation would be necessary and time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Ideas for the Next Assignment:\n",
    "\n",
    "**Dataset Idea:** *e.g., A large dataset of customer reviews (millions of rows).*\n",
    "\n",
    "**MapReduce Application:** *e.g., Use MapReduce to process all reviews in parallel to calculate the average review score per product ID. The mapper would emit `(product_id, score)` and the reducer would calculate the average for each product. This pre-computed data would make visualizing top/bottom products much faster.*\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Idea:** *Your idea here...*\n",
    "\n",
    "**MapReduce Application:** *Your idea here...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
